2020-06-16 08:54:19 Hail: INFO: Running Hail version 0.2.43-d7da1fd14f3c
2020-06-16 08:54:20 SparkContext: WARN: Using an existing SparkContext; some configuration may not take effect.
2020-06-16 08:54:21 root: INFO: RegionPool: initialized for thread 54: Thread-17
2020-06-16 08:54:23 root: ERROR: GoogleJsonResponseException: 400 Bad Request
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Bucket is requester pays bucket but no user project provided.",
    "reason" : "required"
  } ],
  "message" : "Bucket is requester pays bucket but no user project provided."
}
From java.io.IOException: Error accessing gs://ukb-diverse-pops-public/sumstats_release/phenotype_manifest.tsv.bgz
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1911)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1813)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1127)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1095)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1038)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:252)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1186)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1147)
	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)
	at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:118)
	at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:117)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:123)
	at is.hail.expr.ir.TextTableReader$.readMetadata1(TextTableReader.scala:233)
	at is.hail.expr.ir.TextTableReader$$anonfun$readMetadata$1.apply(TextTableReader.scala:225)
	at is.hail.expr.ir.TextTableReader$$anonfun$readMetadata$1.apply(TextTableReader.scala:224)
	at is.hail.HailContext$.maybeGZipAsBGZip(HailContext.scala:411)
	at is.hail.expr.ir.TextTableReader$.readMetadata(TextTableReader.scala:224)
	at is.hail.expr.ir.TextTableReader$.apply(TextTableReader.scala:343)
	at is.hail.expr.ir.TextTableReader$.fromJValue(TextTableReader.scala:350)
	at is.hail.expr.ir.TableReader$.fromJValue(TableIR.scala:101)
	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1241)
	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1217)
	at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$1.apply(Parser.scala:1686)
	at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$1.apply(Parser.scala:1686)
	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1675)
	at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1686)
	at is.hail.backend.spark.SparkBackend$$anonfun$parse_table_ir$1.apply(SparkBackend.scala:512)
	at is.hail.backend.spark.SparkBackend$$anonfun$parse_table_ir$1.apply(SparkBackend.scala:511)
	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)
	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)
	at is.hail.utils.package$.using(package.scala:600)
	at is.hail.annotations.Region$.scoped(Region.scala:18)
	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)
	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229)
	at is.hail.backend.spark.SparkBackend.parse_table_ir(SparkBackend.scala:511)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
{
  "code" : 400,
  "errors" : [ {
    "domain" : "global",
    "message" : "Bucket is requester pays bucket but no user project provided.",
    "reason" : "required"
  } ],
  "message" : "Bucket is requester pays bucket but no user project provided."
}
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:451)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1089)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:549)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:482)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:599)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1905)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1813)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfoInternal(GoogleCloudStorageFileSystem.java:1127)
	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem.getFileInfo(GoogleCloudStorageFileSystem.java:1095)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getFileStatus(GoogleHadoopFileSystemBase.java:1038)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:252)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1186)
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.globStatus(GoogleHadoopFileSystemBase.java:1147)
	at is.hail.io.fs.HadoopFS.glob(HadoopFS.scala:139)
	at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:118)
	at is.hail.io.fs.HadoopFS$$anonfun$globAll$1.apply(HadoopFS.scala:117)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1334)
	at is.hail.io.fs.HadoopFS.globAll(HadoopFS.scala:123)
	at is.hail.expr.ir.TextTableReader$.readMetadata1(TextTableReader.scala:233)
	at is.hail.expr.ir.TextTableReader$$anonfun$readMetadata$1.apply(TextTableReader.scala:225)
	at is.hail.expr.ir.TextTableReader$$anonfun$readMetadata$1.apply(TextTableReader.scala:224)
	at is.hail.HailContext$.maybeGZipAsBGZip(HailContext.scala:411)
	at is.hail.expr.ir.TextTableReader$.readMetadata(TextTableReader.scala:224)
	at is.hail.expr.ir.TextTableReader$.apply(TextTableReader.scala:343)
	at is.hail.expr.ir.TextTableReader$.fromJValue(TextTableReader.scala:350)
	at is.hail.expr.ir.TableReader$.fromJValue(TableIR.scala:101)
	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:1241)
	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:1217)
	at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$1.apply(Parser.scala:1686)
	at is.hail.expr.ir.IRParser$$anonfun$parse_table_ir$1.apply(Parser.scala:1686)
	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1675)
	at is.hail.expr.ir.IRParser$.parse_table_ir(Parser.scala:1686)
	at is.hail.backend.spark.SparkBackend$$anonfun$parse_table_ir$1.apply(SparkBackend.scala:512)
	at is.hail.backend.spark.SparkBackend$$anonfun$parse_table_ir$1.apply(SparkBackend.scala:511)
	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)
	at is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)
	at is.hail.utils.package$.using(package.scala:600)
	at is.hail.annotations.Region$.scoped(Region.scala:18)
	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)
	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:229)
	at is.hail.backend.spark.SparkBackend.parse_table_ir(SparkBackend.scala:511)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)




2020-06-16 08:54:24 SparkContext: INFO: Invoking stop() from shutdown hook
2020-06-16 08:54:24 AbstractConnector: INFO: Stopped Spark@39288e60{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2020-06-16 08:54:24 SparkUI: INFO: Stopped Spark web UI at http://10.0.0.51:4041
2020-06-16 08:54:24 MapOutputTrackerMasterEndpoint: INFO: MapOutputTrackerMasterEndpoint stopped!
2020-06-16 08:54:24 MemoryStore: INFO: MemoryStore cleared
2020-06-16 08:54:24 BlockManager: INFO: BlockManager stopped
2020-06-16 08:54:24 BlockManagerMaster: INFO: BlockManagerMaster stopped
2020-06-16 08:54:24 OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: INFO: OutputCommitCoordinator stopped!
2020-06-16 08:54:24 SparkContext: INFO: Successfully stopped SparkContext
2020-06-16 08:54:24 ShutdownHookManager: INFO: Shutdown hook called
2020-06-16 08:54:24 ShutdownHookManager: INFO: Deleting directory /private/var/folders/56/q7rh2y5x7hj4n359_13dgwxcmtwz20/T/spark-d00e3ce5-5ef9-4502-8d5e-31c7277d811e
2020-06-16 08:54:24 ShutdownHookManager: INFO: Deleting directory /private/var/folders/56/q7rh2y5x7hj4n359_13dgwxcmtwz20/T/spark-117e3f26-9c80-4bd9-93e2-219b01128428/pyspark-34c7d637-9e81-47f2-a39d-dd2595dafead
2020-06-16 08:54:24 ShutdownHookManager: INFO: Deleting directory /private/var/folders/56/q7rh2y5x7hj4n359_13dgwxcmtwz20/T/spark-117e3f26-9c80-4bd9-93e2-219b01128428
